{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c8cb98-aa47-4684-a147-5f3e6ca1c94d",
   "metadata": {},
   "source": [
    "# Employee Attrition Rate Prediction using SageMaker XGBoost\n",
    "This notebook demonstrates how to:\n",
    "- Generate sample employee data\n",
    "- Perform feature engineering\n",
    "- Train an XGBoost model using SageMaker built-in algorithm\n",
    "- Deploy the model and test predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df676769-5f2a-401a-aff1-38a6c671a4f5",
   "metadata": {},
   "source": [
    "## Step 1: Import Required Libraries\n",
    "### This Cell contains TODO 1, please ensure to complete it.\n",
    "This cell imports necessary Python libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15d9925-c228-4b9b-a9c3-4a79cce3a197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Initialize SageMaker session\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "#bucket = session.default_bucket()\n",
    "bucket = f\"jam-sm-sagemaker-bucket-{account_id}\"\n",
    "prefix = 'employee-attrition'\n",
    "\n",
    "# For demonstration, we'll create synthetic data\n",
    "def generate_synthetic_data(n_samples=1000):\n",
    "    np.random.seed(42)\n",
    "    data = {\n",
    "        'Age': np.random.randint(22, 60, n_samples),\n",
    "        'YearsAtCompany': np.random.randint(0, 20, n_samples),\n",
    "        'Salary': np.random.randint(30000, 150000, n_samples),\n",
    "        'JobSatisfaction': np.random.randint(1, 5, n_samples),\n",
    "        'Department': np.random.choice(['HR', 'Sales', 'Engineering', 'Finance'], n_samples),\n",
    "        'OverTime': np.random.choice(['Yes', 'No'], n_samples),\n",
    "        'WorkLifeBalance': np.random.randint(1, 5, n_samples),\n",
    "        'PerformanceRating': np.random.randint(1, 5, n_samples),\n",
    "        'Attrition': np.random.choice(['Yes', 'No'], n_samples, p=[0.16, 0.84])  # 16% attrition rate\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate and display sample data\n",
    "\n",
    "# TODO 1\n",
    "df = # Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "199f17e4-7254-45a4-a913-3bd00624f05e",
   "metadata": {},
   "source": [
    "### Step 1.1: Display top 10 records from the sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddba92b-cb1c-48bb-9765-e7fbbf8a1a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aab9a0-1ea2-4008-93fe-b71f5a0fdcb0",
   "metadata": {},
   "source": [
    "## Step 2: Data Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b22fc18-1103-459b-b3f1-f20042a2518d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(df.describe())\n",
    "\n",
    "# Check class distribution\n",
    "attrition_counts = df['Attrition'].value_counts()\n",
    "print(f\"Attrition distribution:\\n{attrition_counts}\")\n",
    "print(f\"Attrition rate: {attrition_counts['Yes'] / len(df):.2%}\")\n",
    "\n",
    "# Visualize key relationships\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Age vs Attrition\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.boxplot(x='Attrition', y='Age', data=df)\n",
    "plt.title('Age vs Attrition')\n",
    "\n",
    "# Job Satisfaction vs Attrition\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.countplot(x='JobSatisfaction', hue='Attrition', data=df)\n",
    "plt.title('Job Satisfaction vs Attrition')\n",
    "\n",
    "# Department vs Attrition\n",
    "plt.subplot(2, 2, 3)\n",
    "dept_attrition = pd.crosstab(df['Department'], df['Attrition'], normalize='index')\n",
    "dept_attrition['Yes'].sort_values().plot(kind='bar')\n",
    "plt.title('Attrition Rate by Department')\n",
    "plt.ylabel('Attrition Rate')\n",
    "\n",
    "# Overtime vs Attrition\n",
    "plt.subplot(2, 2, 4)\n",
    "overtime_attrition = pd.crosstab(df['OverTime'], df['Attrition'], normalize='index')\n",
    "overtime_attrition['Yes'].plot(kind='bar')\n",
    "plt.title('Attrition Rate by Overtime Status')\n",
    "plt.ylabel('Attrition Rate')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e3867a-eeba-44c3-9159-a90daef33efc",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering\n",
    "### This Cell perfoms feature engineering and splits the data into Training and Test Dataset.\n",
    "### Feature Engineered Data would be uploaded to S3 Bucket.\n",
    "### This Cell contains TODO 2, please ensure to complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97073c88-8992-4470-9715-afc6e33f1e08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X = df.drop('Attrition', axis=1)\n",
    "y = (df['Attrition'] == 'Yes').astype(int)  # Convert to binary\n",
    "\n",
    "# Split data\n",
    "# TODO 2\n",
    "#Split the data into 80% Training and 20% Test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, '''YOUR CODE HERE''', random_state=42, stratify=y)\n",
    "\n",
    "# Define preprocessing for numerical and categorical features\n",
    "numerical_features = ['Age', 'YearsAtCompany', 'Salary', 'JobSatisfaction', 'WorkLifeBalance', 'PerformanceRating']\n",
    "categorical_features = ['Department', 'OverTime']\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', StandardScaler(), numerical_features),\n",
    "        ('cat', OneHotEncoder(drop='first'), categorical_features)\n",
    "    ])\n",
    "\n",
    "# Preprocess the data\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "# Convert to numpy arrays\n",
    "train_features = X_train_processed.toarray() if hasattr(X_train_processed, 'toarray') else X_train_processed\n",
    "test_features = X_test_processed.toarray() if hasattr(X_test_processed, 'toarray') else X_test_processed\n",
    "\n",
    "print(f\"Training data shape: {train_features.shape}\")\n",
    "print(f\"Testing data shape: {test_features.shape}\")\n",
    "\n",
    "# Save the preprocessor for later use with new data\n",
    "import joblib\n",
    "joblib.dump(preprocessor, 'preprocessor.joblib')\n",
    "\n",
    "# Format data for SageMaker XGBoost (CSV format with label in first column)\n",
    "def format_for_xgboost(features, labels):\n",
    "    \"\"\"Format features and labels into CSV format for XGBoost.\"\"\"\n",
    "    if labels is not None:\n",
    "        # For training/validation: label in first column\n",
    "        data = np.hstack((labels.reshape(-1, 1), features))\n",
    "    else:\n",
    "        # For prediction: features only\n",
    "        data = features\n",
    "    return data\n",
    "\n",
    "# Format training and test data\n",
    "train_data = format_for_xgboost(train_features, y_train.values)\n",
    "test_data = format_for_xgboost(test_features, y_test.values)\n",
    "\n",
    "# Upload to S3\n",
    "def upload_to_s3(data, bucket, prefix, filename):\n",
    "    \"\"\"Upload data to S3 in CSV format.\"\"\"\n",
    "    csv_buffer = io.StringIO()\n",
    "    np.savetxt(csv_buffer, data, delimiter=',')\n",
    "    s3_resource = boto3.resource('s3')\n",
    "    s3_resource.Object(bucket, f\"{prefix}/{filename}\").put(Body=csv_buffer.getvalue())\n",
    "    return f\"s3://{bucket}/{prefix}/{filename}\"\n",
    "\n",
    "# Upload data to S3\n",
    "train_s3_path = upload_to_s3(train_data, bucket, prefix, 'train.csv')\n",
    "test_s3_path = upload_to_s3(test_data, bucket, prefix, 'test.csv')\n",
    "\n",
    "print(f\"Training data uploaded to: {train_s3_path}\")\n",
    "print(f\"Test data uploaded to: {test_s3_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082e0ffc-d263-4430-a65c-9be9960be44a",
   "metadata": {},
   "source": [
    "## Step 4: Model Training\n",
    "### This cell would create Estimator object. We would use XGBoost Algorithm for this Task. XGBoost is an Extreme Gradient Boosting Algorithm used for Classification and Regression Problems.\n",
    "### This cell also sets the hyperparameters and defines the input and output data channels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9376d9d-2be7-4280-ae7a-8a7644709676",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Get the container image for XGBoost\n",
    "container = sagemaker.image_uris.retrieve('xgboost', session.boto_region_name, '1.5-1')\n",
    "\n",
    "# Set up the estimator\n",
    "xgb = Estimator(\n",
    "    image_uri=container,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path=f's3://{bucket}/{prefix}/output',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Set hyperparameters\n",
    "xgb.set_hyperparameters(\n",
    "    max_depth=5,\n",
    "    eta=0.2,\n",
    "    gamma=4,\n",
    "    min_child_weight=6,\n",
    "    subsample=0.8,\n",
    "    objective='binary:logistic',\n",
    "    num_round=100,\n",
    "    verbosity=1\n",
    ")\n",
    "\n",
    "# Define data channels\n",
    "train_input = sagemaker.inputs.TrainingInput(\n",
    "    train_s3_path,\n",
    "    content_type='text/csv',\n",
    "    distribution='FullyReplicated'\n",
    ")\n",
    "\n",
    "validation_input = sagemaker.inputs.TrainingInput(\n",
    "    test_s3_path,\n",
    "    content_type='text/csv',\n",
    "    distribution='FullyReplicated'\n",
    ")\n",
    "\n",
    "data_channels = {\n",
    "    'train': train_input,\n",
    "    'validation': validation_input\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8cef819-a91d-4b57-9fec-581aee2c1e89",
   "metadata": {},
   "source": [
    "## Step 4.1: Model Training\n",
    "### Start the Model Training Job\n",
    "### This Cell contains TODO 3, please ensure to complete it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c4c837-fc33-4ab7-b1b8-9fddbcdf5bff",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# TODO 3 - Start the training job\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efe97e2-2fd0-451c-b9ba-a2215d3ddb9f",
   "metadata": {},
   "source": [
    "## Step 5: Deploy Model\n",
    "### After the model gets trained, it gets deployed for making predictions, we are going to deploy the model as a real-time endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710908bd-0598-4535-a85a-b8abc6b7a7c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Deploy the model to an endpoint\n",
    "xgb_predictor = xgb.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m5.large',\n",
    "    serializer=sagemaker.serializers.CSVSerializer()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39505464-2b45-4c2c-9cff-c5d0512efa6f",
   "metadata": {},
   "source": [
    "## Step 5.1: Intialize a variable with the endpoint name, so it can be used for making inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ed70c7-385e-4969-86ba-ce2289b9233a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the endpoint name\n",
    "endpoint_name = xgb_predictor.endpoint_name\n",
    "print(f\"Endpoint Name: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0146f-67c7-4324-8245-254ac0c8acb7",
   "metadata": {},
   "source": [
    "## Step 5.2: Create a method to make predictions on the deployed endpoint. We are using SageMaker Python SDK for creating this method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ec3ed-4ec7-4ee4-b572-2dcb5a551575",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Method 1: Using the SageMaker Python SDK\n",
    "def predict_with_sagemaker_sdk(data, predictor):\n",
    "    \"\"\"Make predictions using the SageMaker Python SDK.\"\"\"\n",
    "    # Ensure data is in the right format for XGBoost built-in algorithm\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        # Preprocess the data\n",
    "        preprocessor = joblib.load('preprocessor.joblib')\n",
    "        processed_data = preprocessor.transform(data)\n",
    "        if hasattr(processed_data, 'toarray'):\n",
    "            processed_data = processed_data.toarray()\n",
    "        \n",
    "        # Convert to CSV format - XGBoost built-in algorithm expects CSV with no header\n",
    "        csv_data = io.StringIO()\n",
    "        np.savetxt(csv_data, processed_data, delimiter=',')\n",
    "        \n",
    "        # Make prediction\n",
    "        response = predictor.predict(csv_data.getvalue())\n",
    "        \n",
    "        # Parse the response - built-in XGBoost may return multiple lines\n",
    "        if isinstance(response, bytes):\n",
    "            response_str = response.decode('utf-8')\n",
    "        else:\n",
    "            response_str = response\n",
    "            \n",
    "        # Split by lines and parse each line as JSON\n",
    "        predictions = []\n",
    "        for line in response_str.strip().split('\\n'):\n",
    "            if line:  # Skip empty lines\n",
    "                predictions.append(float(line))  # XGBoost returns one float per line\n",
    "                \n",
    "        return np.array(predictions)\n",
    "    else:\n",
    "        raise ValueError(\"Input must be a pandas DataFrame\")\n",
    "            \n",
    "    # Convert to CSV string - XGBoost built-in algorithm expects CSV with no header\n",
    "    csv_buffer = io.StringIO()\n",
    "    np.savetxt(csv_buffer, processed_data, delimiter=',')\n",
    "    csv_data = csv_buffer.getvalue()\n",
    "    \n",
    "    \n",
    "    # Create a boto3 client\n",
    "    runtime_client = boto3.client('sagemaker-runtime')\n",
    "    \n",
    "    # Make the prediction\n",
    "    response = runtime_client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name,\n",
    "        ContentType='text/csv',\n",
    "        Body=csv_data\n",
    "    )\n",
    "    \n",
    "    # Parse the response - built-in XGBoost may return multiple lines\n",
    "    response_str = response['Body'].read().decode('utf-8')\n",
    "    \n",
    "    # Split by lines and parse each line\n",
    "    predictions = []\n",
    "    for line in response_str.strip().split('\\n'):\n",
    "        if line:  # Skip empty lines\n",
    "            predictions.append(float(line))  # XGBoost returns one float per line\n",
    "            \n",
    "    return np.array(predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae59db8-9104-4cb2-8b15-750e6ce4fb7a",
   "metadata": {},
   "source": [
    "## Step 5.3: Next we create some sample data for making predictions.\n",
    "### Note the sample data does not include the Label/Target Column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7f2282-31fc-4e41-9f31-edceefa61b91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create sample employee data for prediction\n",
    "sample_employees = pd.DataFrame({\n",
    "    'Age': [35, 42, 29, 55, 38],\n",
    "    'YearsAtCompany': [3, 15, 1, 20, 7],\n",
    "    'Salary': [85000, 120000, 65000, 130000, 95000],\n",
    "    'JobSatisfaction': [3, 4, 2, 5, 3],\n",
    "    'Department': ['Sales', 'Engineering', 'HR', 'Finance', 'Engineering'],\n",
    "    'OverTime': ['Yes', 'No', 'Yes', 'No', 'No'],\n",
    "    'WorkLifeBalance': [2, 4, 2, 3, 3],\n",
    "    'PerformanceRating': [3, 5, 3, 4, 4]\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b788ba8c-8c12-4a67-b054-0615bc557481",
   "metadata": {},
   "source": [
    "## Step 5.4: Now we make the actual prediction\n",
    "### This cell makes predictions by calling the predict_with_sagemaker method created above.\n",
    "### This Cell contains TODO 4, please ensure to complete it. Ensure you write code to predict those employees who are at 20% risk of qutting the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f55c5-22fa-4275-a9ca-e42dc21d5f0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Make predictions using SageMaker SDK\n",
    "sdk_predictions = predict_with_sagemaker_sdk(sample_employees, xgb_predictor)\n",
    "\n",
    "# TODO 4 - Predict Employees who are at 20% or above at the risk of quitting\n",
    "sdk_attrition_risk = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2310ef7a-77ca-4832-a1a4-ae2707accd1b",
   "metadata": {},
   "source": [
    "## Step 5.5: Display the results from making prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b402b6-12f8-43ed-8718-91df0177a8bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Display results\n",
    "results = pd.DataFrame({\n",
    "    'Employee': range(1, len(sample_employees) + 1),\n",
    "    'Age': sample_employees['Age'],\n",
    "    'Department': sample_employees['Department'],\n",
    "    'YearsAtCompany': sample_employees['YearsAtCompany'],\n",
    "    'OverTime': sample_employees['OverTime'],\n",
    "    'Attrition_Probability': sdk_predictions,\n",
    "    'Attrition_Risk': ['High' if risk == 1 else 'Low' for risk in sdk_attrition_risk]\n",
    "})\n",
    "print(\"\\nEmployee Attrition Prediction Results:\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d72439-b39f-4d57-a33a-ac85b34a4b15",
   "metadata": {},
   "source": [
    "## Step 6: Optional (Clean Up)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4962f549-e8d2-4827-a6f4-7f900970664a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up resources when done\n",
    "print(\"\\nCleaning up resources...\")\n",
    "xgb_predictor.delete_endpoint()\n",
    "print(\"Endpoint deleted successfully\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
